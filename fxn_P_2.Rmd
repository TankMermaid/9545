---
title: "P value and functions"
author: "gg"
date: "September 16, 2015"
output: html_document
---
## functions and loops in R

for loops are the essential programming tool after variable assignment

we want to iterate over or through something
eg. what are the means of all row values in a table


if you understand these, most programming is pretty much this paradigm
```{r, echo=TRUE, results='hide'}
# x is a variable that is a matrix
# it is filled with NA, 10 rows and 10 columns
x.mat <- matrix(data=NA, nrow=1000, ncol=10)

# i is a new variable, counter
# 1:nrow(x) from 1 to num rows in x (10)
# you read this for loop as:
# starting at counter 1, do whatever is in the {}
# at the end of the {}, iterate to the next value and repeat
# stop when you run out of values
for(i in 1:nrow(x.mat)){
  x.mat[i,] <- rnorm(ncol(x.mat), 10, 2)
}
# display x.mat (if results='show' in the R code block definition)
x.mat
```
We want to use a for loop to determine the mean of every row. Here we are just diong what we did above, with a different set of commands inside the loop
```{r, echo=TRUE, results='hide'}
x.vec <- vector()
for(i in 1:nrow(x.mat)){
x.vec[i] <- mean(x.mat[i,])
}
1:nrow(x.mat)
x.vec
```
the apply function to do the same thing as the for loop, but it is fully vectorized. The apply function is usually faster than a for loop, it is cleaner to read.

In the second function below we feed it a vector z, that is each column of matrix x.mat. In R when you work on a vector, you do the same thing to every element of the vector. So we have a vector z, and we subtract the mean of vector z from each value of the vector.

```{r, echo=TRUE, results='show'}
# apply is a complete vectorization of the operation
# takes 3 things, (matrix or data.frame, margin, function)
# margin is row (1), or column (2)
# function can be predefined or user defined
# here we use a predefined function: mean()
x.mean <- apply(x.mat, 2, mean)

# make our own function
# this will find the difference between the mean of a column (margin=2), and each value
x.residual <- apply(x.mat,2,function(z){z - mean(z)})

x.mat[1,1]
x.mean[1]
x.residual[1,1]
```
## P values - or the stat's course you needed in 1 hour
P values give the likelihood of measuring a difference as extreme or more extreme than the observed difference if the difference was due to random chance. Note that this is almost never what you actually want to measure. As a biochemist/biologist you are actually interested in the question, `is this biologically relevant' not is it unlikely to have been observed by chance.

As an example, the generic formula for a t-test is the difference between means (of two sets of observations) divided by the standard error of the mean (this can be single, or pooled depending on the specific t-test used). The denominator depends on the number of the samples, and more samples gives a more precise estimate of the standard error of the mean.

In general, what you want to find is instead the effect size, which is how separable the two sets of samples are. The generic formula for effect size is similar, but the denominator is the standard deviation (or pooled standard deviation). This number gives some information about how likely it is that given a random observation that you could place it into group A or B correctly.

The function and histogram below shows the distribution of P values for randomly chosen data. Notice that this is pretty close to a random uniform distribution - which is what we expect.  We have 10 samples (1:5 and 6:10) in x.mat, and 1000 variables in each sample. So we expect 1000 * 0.05 = 50 P values to be below 0.05 by chance alone

```{r, echo=TRUE}
# apply function for t.test
# results of t.test are a list, so we must conver to a number using as.numeric
# test of `significance' of 1:5 vs 6:10 of each row
# doing 1000 tests
x.tt <- apply(x.mat, 1, function(x){as.numeric(t.test(x[1:5], x[6:10])[3])})
# histogram of the p values
# remember this is RANDOM DATA
# no difference is expected
hist(x.tt, breaks=100)
abline(v=0.05, col="red")

# how many are below 0.05 in this instance?
length(which(x.tt < 0.05))

#note how impressive our lowest P value is! Surely this is significant. No it's not, and don't call me Shirley!
min(x.tt)
```
So you see that having a `significant' P value can easily be because of chance alone when you are determining lots of P values. For this reason we actually want to correct our P values when we have many variables (multivariate data or analyses) for the likelihood that the P values are being generated by random data, or from groups with no real difference (same thing). This is done in two ways:

- Bonferroni correction is also called a family wide error rate, and adjusts each P value by multiplying it by the number of hypothesis tests. In our example, each P would be multiplied by 1000, and if it exceeds 1, it becomes 1. The idea here is that P values must be extreme enough to be assured of not arising by chance at the given threshold in the entire dataset.
- Benjamini-Hochberg is also called the false discovery rate, and adjusts each P value by (essentially) their residual rank order Bonferroni adjusment. The idea here is to order the P values from smallest to largest, and to evaluate each P value individually for the likelihood it is a false positive with the function P times (number of tests/(number of corrected ones + 1)). In this approach, each P value can be interpreted as the likelihood that it is a false positive (false discovery).

Note that when this example is run, in general, both the BH and Bonferroni adjusments indicate that no P values are extreme enough to be considered significant.
```{r, echo=TRUE}
# adjust the P values using BH for multiple hypothesis tests (we did 1000)
x.padj <- p.adjust(x.tt, method="BH")
hist(x.padj)

# minimum BH adjusted value
min(x.padj)

x.padj <- p.adjust(x.tt, method="bonferroni")

# minimum Bonferroni adjusted value
min(x.padj)

```

